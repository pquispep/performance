{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se requiere realizar:\n",
    "\n",
    "1. Generar una sábana que nos permita denormalizar la información para futuros análisis.\n",
    "2. Guardar dicha sábana en un archivo de texto.\n",
    "3. Hacer un análisis  que nos indique el volúmen de ventas  que nos permita interactuar con los resultados (OLAP/Pivot en excel).\n",
    "4. Hacer una segmentación de clientes por categoría indicando qué categoría será más probable que compre cada uno de los clientes y generar un archivo de texto con la siguiente estructura:\n",
    "   id_customer Int\n",
    "   category String\n",
    "\n",
    "Requerimientos: \n",
    "      1. Se requiere un (o varios) scripts que realicen la lectura de los archivos y su carga para su análisis.\n",
    "\t     Se dará prioridad si se realiza completamente en python (Jupyter Notebook o scripts convencionales).\n",
    "\t  2. Se lo más ordenado posible.\n",
    "\t  3. No es necesario justificar el método que se empleó en el punto 4 y eres libre de escoger el que sea más\n",
    "\t     adecuado a tus conocimientos.\n",
    "\t  3. Subir tus códigos a GitHub/Bitbucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos las librerias y seteamos la ruta de trabajo\n",
    "import pandas as pd\n",
    "import os\n",
    "z=os.getcwd()\n",
    "os.chdir(\"/Users/pquispe/Downloads/Pruebas_Performance_Linio\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la data de catalogo y asignamos las los headers\n",
    "catalog_df = pd.read_csv(\"catalog.csv\",header=None)\n",
    "catalog_df.columns=[\"sku_code\",\"category\",\"timestamp\"]\n",
    "print(\"catlog \",catalog_df.shape)\n",
    "catalog_df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eliminamos los registros duplicados de catalogo\n",
    "catalog_unico_df=catalog_df.drop_duplicates()\n",
    "print(\"catalog unico \",catalog_unico_df.shape)\n",
    "catalog_unico_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la data de customers y asignamos las los headers\n",
    "customers_df = pd.read_csv(\"customers.csv\",header=None)\n",
    "customers_df.columns=[\"id_cliente\",\"nombre\",\"email\",\"timestamp\"]\n",
    "print(\"customers \",customers_df.shape)\n",
    "customers_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos la data de ventas y asignamos las los headers\n",
    "ventas_2019_df = pd.read_csv(\"ventas_2019.csv\",header=None)\n",
    "ventas_2019_df.columns=[\"id_venta\",\"id_customer\",\"sku_code\",\"numero_de_orden\",\"precio_unitario\",\"unidades\",\"importe\",\"timestamp\"]\n",
    "print(\"ventas \",ventas_2019_df.shape)\n",
    "ventas_2019_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unimos las fuentes para obtener la sabana desnomalizada\n",
    "union_pre01 = pd.merge(left=ventas_2019_df,right=customers_df, how='left', left_on='id_customer', right_on='id_cliente')\n",
    "print(\"union_pre01 \",union_pre01.shape)\n",
    "\n",
    "union = pd.merge(left=union_pre01,right=catalog_unico_df, how='left', left_on='sku_code', right_on='sku_code')\n",
    "print(\"union \",union.shape)\n",
    "\n",
    "\n",
    "#exportamos el resultado de la union de fuentes\n",
    "union.to_csv(\"salida.csv\",index=False)\n",
    "\n",
    "union.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Generando data data excel\n",
    "union['timestamp_x']=pd.to_datetime(union['timestamp_x'])\n",
    "union['anio_v']=union['timestamp_x'].dt.year\n",
    "union['mes_v']=union['timestamp_x'].dt.month\n",
    "df1=pd.DataFrame(union.groupby(['category','nombre','anio_v','mes_v','timestamp_x'])[['unidades','importe']].sum())\n",
    "\n",
    "union.to_csv(\"salida_para_excel.csv\",index=False)\n",
    "\n",
    "df1.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "encoder=LabelEncoder()\n",
    "\n",
    "union['categoria']=encoder.fit_transform(union.category.values)\n",
    "union.head(10)\n",
    "\n",
    "x=union['id_customer']\n",
    "y=union['categoria']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x = mean_data = np.array(x)\n",
    "X=x[:,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    mlr=MLPRegressor(solver=\"lbfgs\",alpha=1e-5,hidden_layer_sizes=(3,3), random_state=1)\n",
    "    mlr.fit(X_train, y_train)\n",
    "    print(mlr.score(X_train, y_train))\n",
    "    if mlr.score(X_train,y_train) > 0.45:\n",
    "        break\n",
    " \n",
    "print('Categoria para ')\n",
    "print(mlr.predict(np.array(57475).reshape(1, 1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
